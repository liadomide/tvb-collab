{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB PIPELINE: constructing **Brain Network Models** from empirical MRI data\n",
    " \n",
    "## The pipeline will generate structural connectomes, region-average fMRI time series and functional connectomes for simulation within The Virtual Brain\n",
    "\n",
    "### This pipeline is based on the Apps\n",
    "* thevirtualbrain/tvb-pipeline-sc:1.0 (dwMRI preprocessing, tractography)\n",
    "* thevirtualbrain/tvb-pipeline-fmriprep:1.0 (fMRI preprocessing)\n",
    "* thevirtualbrain/tvb_converter (structural connectome, region-wise fMRI, functional connectivity, TVB input data set)\n",
    "\n",
    "### In this tutorial you will learn how to ...\n",
    "* ...upload a BIDS data set to TVB-Pipeline JupyterHub\n",
    "* ...import it into a Jupyter notebook\n",
    "* ...upload it to a supercomputer via PyUnicore\n",
    "* ...create and execute batch job scripts for the supercomputer that execute the pipeline\n",
    "* ...download the results to the notebook\n",
    "* ...from there to your computer.\n",
    "\n",
    "### Authors / Feedback \n",
    "michael.schirner@charite.de  \n",
    "petra.ritter@charite.de  \n",
    "\n",
    "### Acknowledgments\n",
    "Thank you to Paul Triebkorn for co-developing the original version of this script on Collab 1.   \n",
    "\n",
    "Thank you to the developers of MRtrix3_Connectome (Dr. Robert Smith, Florey) [1] and fmriprep [2] for creating excellent open source software that form the basis for this workflow.   \n",
    "\n",
    "\n",
    "[1] https://hub.docker.com/r/bids/mrtrix3_connectome, \n",
    "https://github.com/BIDS-Apps/MRtrix3_connectome\n",
    "   \n",
    "   \n",
    "[2] Esteban, O., Markiewicz, C. J., Blair, R. W., Moodie, C. A., Isik, A. I., Erramuzpe, A., Kent, J. D., Goncalves, M., DuPre, E., Snyder, M., Oya, H., Ghosh, S. S., Wright, J., Durnez, J., Poldrack, R. A., & Gorgolewski, K. J. (2019). fMRIPrep: a robust preprocessing pipeline for functional MRI. Nature Methods. https://doi.org/10.1038/s41592-018-0235-4\n",
    "https://hub.docker.com/r/poldracklab/fmriprep/, \n",
    "https://github.com/poldracklab/fmriprep\n",
    "\n",
    "## 1. Create EBRAINS Collab and upload BIDS data set\n",
    "\n",
    "1. Input data **must** be in BIDS format to run the following operations. Check out the BIDS format [here](https://bids.neuroimaging.io/) for more info. Hint: there are programs that transform data from other formats into BIDS.\n",
    "2. Navigate to persisted-data folder\n",
    "3. Click on the \"Create a Collab\" button and fill out the form.\n",
    "4. Create one folder for your notebooks and one for your data (New -> Folder). In the following we will assume that you called the folders \"notebooks\" and \"data\".\n",
    "6. Duplicate this Ipython notebook and move it to your newly created folder \"notebooks\" in your persisted notebooks folder: persisted-data/notebooks.\n",
    "7. Upload your BIDS data set as a zip file into \"persisted-data/data\".\n",
    "8. Make sure that the two folders were successfully created and that the pipeline notebook and your input data were successfully uploaded into these folders.\n",
    "9. Make sure that you adapt the paths to the BIDS data set and for output data according to your new persisted folders.\n",
    "\n",
    "In this example we uploaded the file\n",
    "```\n",
    "dataset.zip\n",
    "```\n",
    "\n",
    "into the folder\n",
    "```\n",
    "/data\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Note that we specify two paths:\n",
    "* one to the BIDS MRI input data set and\n",
    "* one to the FreeSurfer license file, which we need to run fmriprep. If you do not have already a license file, you can obtain it following the instructions here: https://surfer.nmr.mgh.harvard.edu/fswiki/License\n",
    "\n",
    "To upload the data into your personal persisted folder go to the main TVB-Pipeline JupyerHub page and click on the top-right \"Upload\" button. Note that you must be logged in and you must be the creator/owner of the Collab to be able to do that. Once you are in your persisted folder, use the buttons to create the folders 'data' and 'FreeSurfer_license' (or name them however you want, just make sure the names are correct when we build the path below) and upload the zipped BIDS data set into the former and the license.txt from FreeSurfer into the latter.![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/pipeline/data/dataset.zip\n",
      "/home/jovyan/pipeline/data/license.txt\n"
     ]
    }
   ],
   "source": [
    "# get path of home folder\n",
    "import os\n",
    "home = os.getenv('HOME')\n",
    "\n",
    "workspace = \"pipeline\"\n",
    "\n",
    "# paths for persisted and transient drives\n",
    "transient_drive = 'data'\n",
    "persisted_drive = 'persisted-data'\n",
    "\n",
    "# filename\n",
    "dataset = 'dataset.zip'\n",
    "\n",
    "# full path\n",
    "full_path = os.path.join(home, workspace, transient_drive, dataset)\n",
    "print(full_path)\n",
    "\n",
    "# Freesurfer license file folder\n",
    "full_fs_path = os.path.join(home, workspace, transient_drive, 'license.txt')\n",
    "print(full_fs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the file is really there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(full_path))\n",
    "print(os.path.exists(full_fs_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command returns false something went wrong. Note: you can use Bash commands like `ls` and `cd` to navigate in the file system and look for the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to upload our brain model to the supercomputer. Therefore, we create a PyUnicore client.\n",
    "\n",
    "## 3. Upload BIDS data set to supercomputer\n",
    "First, we update PyUnicore, if necessary. Then, we import it. Finally, we connect with Piz Daint. To see which other supercomputers are there, and to learn their ID run the commented \n",
    "```\n",
    "r.site_urls\n",
    "```\n",
    "\n",
    "To select a different supercomputer replace the supercomputer identifier string in\n",
    "```\n",
    "site_client = r.site('DAINT-CSCS')\n",
    "```\n",
    "with your preferred supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pyunicore library\n",
    "import pyunicore.client as unicore_client\n",
    "\n",
    "tr = unicore_client.Transport(get_token())\n",
    "r = unicore_client.Registry(tr, unicore_client._HBP_REGISTRY_URL)\n",
    "site_client = r.site('DAINT-CSCS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start an \"empty\" interactive job to get a workspace on Piz Daint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = {}\n",
    "job = site_client.new_job(job_description)\n",
    "storage = job.working_dir\n",
    "#storage.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check the contents of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it's empty. If it weren't empty we can remove files or folders with `storage.rm(filename)` or `storage.rmdir(foldername)`. Run `help(storage)` for more information.\n",
    "\n",
    "Before uploading the BIDS ZIP file, let's look in which folder we landed on the supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/1fd2bf1c-438f-400a-a3fc-ab8644f679ab/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = (storage.properties['mountPoint']).encode('ascii')\n",
    "working_dir = working_dir.decode('utf-8') # we get a \"byte\"-type but need a string type\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We will need the path to the working directory later.\n",
    "\n",
    "Now, let's copy the ZIP file to the supercomputer and check whether it arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset.zip': PathFile: dataset.zip}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.upload(input_name = full_path, destination = dataset)\n",
    "storage.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset.zip` is there -- the upload was successful.\n",
    "Now we need to extract the ZIP file. We will use the program `unzip` for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use this client to execute the unzip command and take a quick look into the folder whether `unzip` started working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_result = site_client.execute(\"unzip \" + working_dir + dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execute command launches unzip and forwards the result into a new folder. Let's find out the path of the folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_folder = exec_result.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_handle = exec_result.working_dir\n",
    "base_folder = base_folder.decode('utf-8')\n",
    "base_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'stdout': PathFile: stdout,\n",
       " '__MACOSX/': PathDir: __MACOSX/}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result.working_dir.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Unzip` worked, the folder `dataset` has been created (along with some UNICORE related output files and everything written to stdout/stderr during the command).  \n",
    "\n",
    "## 4. Define environment variables\n",
    "\n",
    "We have a working directory on the supercomputer and our data is there. Let's define some environment variables with important folder paths that we can use to call the BIDS Apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvb_output = \"tvb_converter_workdir\"# output folder name\n",
    "\n",
    "input_dir = base_folder + \"dataset\" # the name of the folder extracted from the ZIP file\n",
    "output_dir = base_folder + tvb_output # full path to output folder\n",
    "mrtrix_output = output_dir + \"/mrtrix_output\"\n",
    "\n",
    "fmriprep_output = output_dir + \"/fmriprep_output\"\n",
    "fmriprep_workdir = fmriprep_output + \"/tmp\"\n",
    "tvb_output = output_dir + \"/TVB_output\"\n",
    "tvb_workdir = tvb_output + \"/tmp\"\n",
    "\n",
    "participant_label = \"CON03\" # BIDS <sub-XXX> keyword -- only specify XXX, not the \"sub-\" prefix\n",
    "session_label = \"postop\" # BIDS <ses-XXX> keyword -- only specify XXX, not the \"ses-\" prefix\n",
    "parcellation = \"desikan\" # parcellation atlas for SC and FC -- check out MRtrix3_connectome github page for available options\n",
    "n_cpus = \"36\" # how many CPUs does your HPC node have? We'll set the number of parallel threads accordingly\n",
    "\n",
    "task_name=\"ArchiSocial\" # name of task fmri, as specified in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run BIDS App `thevirtualbrain/tvb-pipeline-sc` for dwMRI preprocessing\n",
    "\n",
    "Here we run the container `thevirtualbrain/tvb-pipeline-sc` two times on the input data. The first time it performs dwMRI preprocessing, the second time it performs the main tractography.    \n",
    "\n",
    "The first time we run it with the command-line option \"participant1\", which does: DWI: Denoising; Gibbs ringing removal; motion, eddy current and EPI distortion correction and outlier detection & replacement; brain masking, bias field correction and intensity normalisation; rigid-body registration & transformation to T1-weighted image. T1-weighted image: bias field correction; brain masking.\n",
    "\n",
    "To run the contaienr we create a SLURM batch job script for launching the `tvb-pipeline-sc` BIDS App. Instead of directly starting a job on the batch system, we use PyUnicore to submit a job on the login node, which in turn submits a job for the batch system. This gives us a greater flexibility to configure our job, we don't have to learn so much PyUnicore (although it's great!) and are failsafe if PyUnicore misses bindings for certain job managers. Note that before we run the container, we make sure that the image is up to date, or, if non-existent, gets pulled for the first time.\n",
    "\n",
    "`sarus run <container_name>`\n",
    "is the standard way of running a container. Here, we additionally use the mount command to directly mount the input and the output folders into the container's filesystem's top-level directories /input and /output.\n",
    "\n",
    "Note that if no ```--participant_label``` and ```--session_label``` arguments are used then all subjects in all sessions will be processed. In this example we use the ```--participant_label``` argument, but not the ```--session_label``` argument. Nevertheless, output folder/file naming schemas will always depend on both ```<sub-XXX>``` and ```<ses-XXX>``` keywords (the latter only if \"sessions\" exist in the input data set).\n",
    "\n",
    "For in-depth instructions for the BIDS App `tvb-pipeline-sc` please refer to the help pages of the original MRtrix3_connectome workflow at Docker Hub and Github:  \n",
    "https://hub.docker.com/r/bids/mrtrix3_connectome/\n",
    "\n",
    "https://github.com/BIDS-Apps/MRtrix3_connectome/\n",
    "\n",
    "\n",
    "For an in-depth discussion of Sarus usage, check out this documentation:  \n",
    "https://user.cscs.ch/tools/containers/sarus/\n",
    "\n",
    "### Please note: There seems to be a strange bug in Sarus: sometimes the pull command   \n",
    "\n",
    "### `srun -C mc --time=03:00:00 sarus pull thevirtualbrain/tvb-pipeline-sc:1.0`   \n",
    "\n",
    "### works and sometimes not. It's often easier to just log into your supercomputing account via SSH and run the pull command in the shell. Once the image with the correct tag was pulled it doesn't need to be re-pulled. Sometimes the pull works if the command was issued multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/pipeline/data/job_script\n"
     ]
    }
   ],
   "source": [
    "# ADJUSTABLE PARAMETERS\n",
    "################################################\n",
    "wall_time = \"23:59:00\" # ADJUST wall time of job\n",
    "job_script = \"job_script\"\n",
    "################################################\n",
    "\n",
    "job_script_path = os.path.join(home, workspace, transient_drive, job_script)\n",
    "print(job_script_path)\n",
    "\n",
    "# create job_script with bash commands\n",
    "# this script will get forwarded to the supercomputer and there run the pipeline\n",
    "with open(job_script_path, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash -l\\n\") \n",
    "    f.write(\"#SBATCH --time=\" + wall_time + \"\\n\")\n",
    "    f.write(\"#SBATCH --output=slurm-\" + job_script + \".out\\n\")\n",
    "    f.write(\"#SBATCH --nodes=1\\n\")\n",
    "    f.write(\"#SBATCH --ntasks-per-core=1\\n\")    \n",
    "    f.write(\"#SBATCH --ntasks-per-node=1\\n\")\n",
    "    f.write(\"#SBATCH --cpus-per-task=\" + n_cpus + \"\\n\")\n",
    "    f.write(\"#SBATCH --partition=normal\\n\")\n",
    "    f.write(\"#SBATCH --constraint=mc\\n\")\n",
    "    f.write(\"#SBATCH --hint=nomultithread\\n\") # disable hyperthreading such that all cores become available for multithreading\n",
    "    f.write(\"export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\\n\")\n",
    "    f.write(\"module load /apps/daint/UES/easybuild/modulefiles/daint-mc\\n\")\n",
    "    f.write(\"module load /apps/daint/system/modulefiles/sarus/1.1.0\\n\\n\")\n",
    "    f.write(\"mkdir -p \" + output_dir + \" \" + mrtrix_output + \" \" + fmriprep_output + \" \" + fmriprep_workdir + \" \" + tvb_output + \" \" + tvb_workdir + \"\\n\\n\")    \n",
    "    f.write(\"sarus pull thevirtualbrain/tvb-pipeline-sc:1.0\\n\\n\")\n",
    "    \n",
    "    f.write(\"srun sarus run \" + \n",
    "            \"--mount=type=bind,source=$HOME,target=$HOME \" + \n",
    "            \"--mount=type=bind,source=\" + input_dir + \",target=/BIDS_dataset \" +\n",
    "            \"--mount=type=bind,source=\" + mrtrix_output + \",target=/output \" +\n",
    "            \"thevirtualbrain/tvb-pipeline-sc:1.0 \" + \n",
    "            \"/BIDS_dataset /output participant1 --participant_label \" + participant_label + \" -skip\")\n",
    "    \n",
    "    \n",
    "#    f.write(\"srun sarus run \" + \n",
    "#            \"--mount=type=bind,source=$HOME,target=$HOME \" + \n",
    "#            \"--mount=type=bind,source=\" + input_dir + \",target=/BIDS_dataset \" +\n",
    "#            \"--mount=type=bind,source=\" + mrtrix_output + \",target=/mrtrix3_out \" +\n",
    "#            \"--entrypoint \" + # necessary for sarus, to overwrite default entrypoint\n",
    "#            \"thevirtualbrain/tvb-pipeline-sc:1.0 python -c \" + \n",
    "#                \"\\\"from mrtrix3 import app; app.cleanup=False; import sys; sys.argv=\" + \n",
    "#                    \"'/mrtrix3_connectome.py /BIDS_dataset /mrtrix3_out participant1 \" + \n",
    "#                    \"--participant_label \" + participant_label + \" -skip \"\n",
    "#                    \" --output_verbosity 2 --template_reg ants --n_cpus $SLURM_CPUS_PER_TASK --debug'\" \n",
    "#                    \".split(); execfile('/mrtrix3_connectome.py')\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the job script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash -l\n",
      "#SBATCH --time=23:59:00\n",
      "#SBATCH --output=slurm-job_script.out\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --ntasks-per-core=1\n",
      "#SBATCH --ntasks-per-node=1\n",
      "#SBATCH --cpus-per-task=36\n",
      "#SBATCH --partition=normal\n",
      "#SBATCH --constraint=mc\n",
      "#SBATCH --hint=nomultithread\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
      "module load /apps/daint/UES/easybuild/modulefiles/daint-mc\n",
      "module load /apps/daint/system/modulefiles/sarus/1.1.0\n",
      "\n",
      "mkdir -p /scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir /scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/mrtrix_output /scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output /scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output/tmp /scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/TVB_output /scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/TVB_output/tmp\n",
      "\n",
      "sarus pull thevirtualbrain/tvb-pipeline-sc:1.0\n",
      "\n",
      "srun sarus run --mount=type=bind,source=$HOME,target=$HOME --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/dataset,target=/BIDS_dataset --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/mrtrix_output,target=/output thevirtualbrain/tvb-pipeline-sc:1.0 /BIDS_dataset /output participant1 --participant_label CON03 -skip\n"
     ]
    }
   ],
   "source": [
    "with open(job_script_path, 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "## 6. Upload SLURM script to supercomputer\n",
    "\n",
    "Here we use the same upload function as used previously for the brain model data followed by a quick check whether the file arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'job_script': PathFile: job_script,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'stdout': PathFile: stdout,\n",
       " '__MACOSX/': PathDir: __MACOSX/}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.upload(input_name=job_script_path, destination = job_script)\n",
    "wd_handle.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a file with the name stored in the variable `job_script` exists, the upload worked.\n",
    "\n",
    "## 7. Launching step 1 of thevirtualbrain/tvb-pipeline-sc on the supercomputer\n",
    "\n",
    "Now we will run the first out of the three BIDS Apps: mrtrix3_connectome.  \n",
    "We do this by executing the the SLURM command `sbatch <job_script>`, which will evaluate our batch file and generate a job out of it that is added to the queue.\n",
    "After we executed the job, we extract the working directory of this job (which will again be a new directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/9c2efbca-06e8-4b0b-adb2-70f33d4ec774/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrtrix_job = site_client.execute(\"sbatch \" + base_folder + job_script)\n",
    "wd_mrtrix_res = mrtrix_job.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_mrtrix_res = wd_mrtrix_res.decode('utf-8')\n",
    "wd_mrtrix_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have two important folder handles and associated folder paths now. The following table gives an overview over handles, associated paths, and their contents.\n",
    "\n",
    "\n",
    "Handle | Path variable | Description\n",
    ":---: | :---: | :---:\n",
    "`wd_handle` | `base_folder` | BIDS input, pipeline output\n",
    "`mrtrix_job` | `wd_mrtrix_res` | SLURM job meta output\n",
    "\n",
    "\n",
    "You can use the handles and path variables to access files and folders contained in the respective folders, which may be helpful, e.g. for debugging or to download other results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Checking the result of step 1 of thevirtualbrain/tvb-pipeline-sc\n",
    "\n",
    "The first step should run relatively quickly (~ 1h) after the job has started.\n",
    "The second step however will take some time, depending on parameters like: number of generated tracks, parcellation, resolution of imaging data, number of parallel threads, etc.\n",
    "\n",
    "Let's check whether step 1 finished successfully by inspecting the last lines of the SLURM meta output, which gives us information related to the job execution.\n",
    "\n",
    "Note that after the job was submitted to the queue it usually takes a while until the job starts. Until then you will receive a \"404 Client Error: Not found for URL\" error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Command:  mrgrid dwi_iter2.mif crop dwi_crop.mif -mask dwi_mask_iter2.mif -uniform -3\\n',\n",
       " b'Command:  mrgrid dwi_mask_iter2.mif crop mask_crop.mif -mask dwi_mask_iter2.mif -uniform -3\\n',\n",
       " b'mrtrix3_connectome.py: Generating contrast-matched images for inter-modal registration between DWIs and T1\\n',\n",
       " b'Command:  dwiextract dwi_crop.mif -bzero - | mrcalc - 0.0 -max - | mrmath - mean -axis 3 dwi_meanbzero.mif\\n',\n",
       " b'Command:  mrcalc 1 dwi_meanbzero.mif -div mask_crop.mif -mult - | mrhistmatch nonlinear - T1.mif dwi_pseudoT1.mif -mask_input mask_crop.mif -mask_target T1_mask.mif\\n',\n",
       " b'Command:  mrcalc 1 T1.mif -div T1_mask.mif -mult - | mrhistmatch nonlinear - dwi_meanbzero.mif T1_pseudobzero.mif -mask_input T1_mask.mif -mask_target mask_crop.mif\\n',\n",
       " b'mrtrix3_connectome.py: Performing registration between DWIs and T1\\n',\n",
       " b'Command:  mrregister dwi_pseudoT1.mif T1.mif -type rigid -mask1 mask_crop.mif -mask2 T1_mask.mif -rigid rigid_pseudoT1_to_T1.txt\\n',\n",
       " b'Command:  mrregister dwi_meanbzero.mif T1_pseudobzero.mif -type rigid  -mask1 mask_crop.mif -mask2 T1_mask.mif -rigid rigid_bzero_to_pseudobzero.txt\\n',\n",
       " b'Command:  transformcalc rigid_pseudoT1_to_T1.txt rigid_bzero_to_pseudobzero.txt average rigid_dwi_to_T1.txt\\n',\n",
       " b'Command:  mrtransform dwi_crop.mif dwi_crop_transform.mif -linear rigid_dwi_to_T1.txt\\n',\n",
       " b'Command:  mrtransform mask_crop.mif mask_crop_transform.mif -linear rigid_dwi_to_T1.txt\\n',\n",
       " b'mrtrix3_connectome.py: Processing completed for session \"sub-CON03_ses-postop\"; writing results to output directory\\n',\n",
       " b\"Function: os.makedirs('/output/sub-CON03/ses-postop')\\n\",\n",
       " b\"Function: os.makedirs('/output/sub-CON03/ses-postop/anat')\\n\",\n",
       " b\"Function: os.makedirs('/output/sub-CON03/ses-postop/dwi')\\n\",\n",
       " b'Command:  mrconvert dwi_crop_transform.mif /output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_desc-preproc_dwi.nii.gz -export_grad_fsl /output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_desc-preproc_dwi.bvec /output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_desc-preproc_dwi.bval -strides +1,+2,+3,+4\\n',\n",
       " b'Command:  mrconvert mask_crop_transform.mif /output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_desc-brain_mask.nii.gz -datatype uint8 -strides +1,+2,+3\\n',\n",
       " b\"Function: shutil.copytree('eddyqc', '/output/sub-CON03/ses-postop/dwi/eddyqc')\\n\",\n",
       " b'Command:  mrconvert T1.mif /output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-preproc_T1w.nii.gz -strides +1,+2,+3\\n',\n",
       " b'Command:  mrconvert T1_mask.mif /output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-brain_mask.nii.gz -datatype uint8 -strides +1,+2,+3\\n',\n",
       " b'mrtrix3_connectome.py: Deleting scratch directory /mrtrix3_connectome.py-tmp-W6850B/\\n',\n",
       " b' \\n',\n",
       " b'Batch Job Summary Report for Job \"job_script\" (25209392) on daint\\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'             Submit            Eligible               Start                 End    Elapsed  Timelimit \\n',\n",
       " b'------------------- ------------------- ------------------- ------------------- ---------- ---------- \\n',\n",
       " b'2020-08-24T10:54:44 2020-08-24T10:54:44 2020-08-24T10:54:44 2020-08-24T12:03:36   01:08:52   23:59:00 \\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'Username    Account     Partition   NNodes   Energy\\n',\n",
       " b'----------  ----------  ----------  ------  --------------\\n',\n",
       " b'bp000309    ich012      normal           1  571.90K joules\\n',\n",
       " b' \\n',\n",
       " b'This job did not utilize any GPUs\\n',\n",
       " b' \\n',\n",
       " b'----------------------------------------------------------\\n',\n",
       " b'Scratch File System        Files       Quota\\n',\n",
       " b'--------------------  ----------  ----------\\n',\n",
       " b'/scratch/snx3000             341     1000000\\n',\n",
       " b' \\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_output = mrtrix_job.working_dir.stat(\"slurm-\" + job_script + \".out\").raw().readlines()\n",
    "slurm_output[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the job finished successfully, the last few lines of the output should contain a batch job summary result, looking like this:\n",
    "```\n",
    " b'mrtrix3_connectome.py: Deleting scratch directory /mrtrix3_connectome.py-tmp-MB587V/\\n',\n",
    " b' \\n',\n",
    " b'Batch Job Summary Report for Job \"job_script\" (23232141) on daint\\n',\n",
    " b'-----------------------------------------------------------------------------------------------------\\n',\n",
    " b'             Submit            Eligible               Start                 End    Elapsed  Timelimit \\n',\n",
    " b'------------------- ------------------- ------------------- ------------------- ---------- ---------- \\n',\n",
    " b'2020-06-10T13:58:04 2020-06-10T13:58:04 2020-06-10T13:58:04 2020-06-10T15:07:05   01:09:01   23:59:00 \\n',\n",
    " b'-----------------------------------------------------------------------------------------------------\\n',\n",
    " b'Username    Account     Partition   NNodes   Energy\\n',\n",
    " b'----------  ----------  ----------  ------  --------------\\n',\n",
    " b'bp000225    ich012      normal           1  589.89K joules\\n',\n",
    " b' \\n',\n",
    " b'This job did not utilize any GPUs\\n',\n",
    " b' \\n',\n",
    " b'----------------------------------------------------------\\n',\n",
    " b'Scratch File System        Files       Quota\\n',\n",
    " b'--------------------  ----------  ----------\\n',\n",
    " b'/scratch/snx3000            6010     1000000\\n',\n",
    " b' \\n']\n",
    " ```\n",
    " \n",
    "\n",
    "\n",
    "## 9. The second step of thevirtualbrain/tvb-pipeline-sc\n",
    "\n",
    "\n",
    "After we made sure that the job finished we now run the second part of the thevirtualbrain/tvb-pipeline-sc workflow, which performs:\n",
    "DWI: Response function estimation; FOD estimation. T1-weighted image (if -parcellation is not none): Tissue segmentation; grey matter parcellation. Combined (if -parcellation is not none, or -streamlines is provided): Whole-brain streamlines tractography; SIFT2; connectome construction.\n",
    "\n",
    "\n",
    "#### We set the following options:\n",
    "\n",
    "+ **--parcellation**<br>The choice of connectome parcellation scheme (compulsory for participant2-level analysis); options are: aal, aal2, brainnetome246fs, brainnetome246mni, craddock200, craddock400, desikan, destrieux, hcpmmp1, none, perry512, yeo7fs, yeo7mni, yeo17fs, yeo17mni.\n",
    "\n",
    "+ **--participant_label**<br>The label(s) of the participant(s) that should be analyzed.\n",
    "\n",
    "+ **-n/--n_cpus number**<br>use this number of threads in multi-threaded applications (set to 0 to disable multi-threading).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/pipeline/data/job_script2\n",
      "#!/bin/bash -l\n",
      "#SBATCH --time=23:59:00\n",
      "#SBATCH --output=slurm-job_script2.out\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --ntasks-per-core=1\n",
      "#SBATCH --ntasks-per-node=1\n",
      "#SBATCH --cpus-per-task=36\n",
      "#SBATCH --partition=normal\n",
      "#SBATCH --constraint=mc\n",
      "#SBATCH --hint=nomultithread\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
      "module load /apps/daint/UES/easybuild/modulefiles/daint-mc\n",
      "module load /apps/daint/system/modulefiles/sarus/1.1.0\n",
      "\n",
      "srun sarus run --mount=type=bind,source=$HOME,target=$HOME --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/dataset,target=/BIDS_dataset --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/mrtrix_output,target=/output thevirtualbrain/tvb-pipeline-sc:1.0 /BIDS_dataset /output participant2 --parcellation desikan --n_cpus $SLURM_CPUS_PER_TASK --participant_label CON03 -skip\n"
     ]
    }
   ],
   "source": [
    "# ADJUSTABLE PARAMETERS\n",
    "################################################\n",
    "wall_time = \"23:59:00\" # ADJUST wall time of job\n",
    "job_script = \"job_script2\"\n",
    "################################################\n",
    "\n",
    "job_script_path = os.path.join(home, workspace, transient_drive, job_script)\n",
    "print(job_script_path)\n",
    "\n",
    "# create job_script with bash commands\n",
    "# this script will get forwarded to the supercomputer and there run the pipeline\n",
    "with open(job_script_path, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash -l\\n\") \n",
    "    f.write(\"#SBATCH --time=\" + wall_time + \"\\n\")\n",
    "    f.write(\"#SBATCH --output=slurm-\" + job_script + \".out\\n\")\n",
    "    f.write(\"#SBATCH --nodes=1\\n\")\n",
    "    f.write(\"#SBATCH --ntasks-per-core=1\\n\")    \n",
    "    f.write(\"#SBATCH --ntasks-per-node=1\\n\")\n",
    "    f.write(\"#SBATCH --cpus-per-task=\" + n_cpus + \"\\n\")\n",
    "    f.write(\"#SBATCH --partition=normal\\n\")\n",
    "    f.write(\"#SBATCH --constraint=mc\\n\")\n",
    "    f.write(\"#SBATCH --hint=nomultithread\\n\") # disable hyperthreading such that all cores become available for multithreading\n",
    "    f.write(\"export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\\n\")\n",
    "    f.write(\"module load /apps/daint/UES/easybuild/modulefiles/daint-mc\\n\")\n",
    "    f.write(\"module load /apps/daint/system/modulefiles/sarus/1.1.0\\n\\n\")\n",
    "    \n",
    "    f.write(\"srun sarus run \" + \n",
    "            \"--mount=type=bind,source=$HOME,target=$HOME \" + \n",
    "            \"--mount=type=bind,source=\" + input_dir + \",target=/BIDS_dataset \" +\n",
    "            \"--mount=type=bind,source=\" + mrtrix_output + \",target=/output \" +\n",
    "            \"thevirtualbrain/tvb-pipeline-sc:1.0 \" + \n",
    "            \"/BIDS_dataset /output participant2 --parcellation desikan --n_cpus $SLURM_CPUS_PER_TASK --participant_label \" + participant_label + \" -skip\")\n",
    "    \n",
    "    \n",
    "with open(job_script_path, 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'job_script': PathFile: job_script,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'stdout': PathFile: stdout,\n",
       " 'job_script2': PathFile: job_script2,\n",
       " '__MACOSX/': PathDir: __MACOSX/,\n",
       " 'tvb_converter_workdir/': PathDir: tvb_converter_workdir/}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.upload(input_name=job_script_path, destination = job_script)\n",
    "wd_handle.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...launch it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/8957b17c-c203-4968-bda6-af4f69f4e9d1/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrtrix_job2 = site_client.execute(\"sbatch \" + base_folder + job_script)\n",
    "wd_mrtrix_res2 = mrtrix_job2.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_mrtrix_res2 = wd_mrtrix_res2.decode('utf-8')\n",
    "wd_mrtrix_res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle | Path variable | Description\n",
    ":---: | :---: | :---:\n",
    "`wd_handle` | `base_folder` | BIDS input, pipeline output\n",
    "`mrtrix_job` | `wd_mrtrix_res` | SLURM job meta output SC 1st part\n",
    "`mrtrix_job2` | `wd_mrtrix_res2` | SLURM job meta output SC 2nd part\n",
    "\n",
    "### ...and check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Command:  tck2connectome tractogram.tck parc.mif meanlength.csv -tck_weights_in weights.csv -scale_length -stat_edge mean\\n',\n",
       " b'mrtrix3_connectome.py: Processing for session \"sub-CON03_ses-postop\" completed; writing results to output directory\\n',\n",
       " b\"Function: os.makedirs('/output/sub-CON03/ses-postop/connectome')\\n\",\n",
       " b\"Function: os.makedirs('/output/sub-CON03/ses-postop/tractogram')\\n\",\n",
       " b'Command:  mrconvert T1.mif /output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-preproc_T1w.nii.gz  -strides +1,+2,+3 -clear_property comments\\n',\n",
       " b\"Function: shutil.copyfile('meanlength.csv', '/output/sub-CON03/ses-postop/connectome/sub-CON03_ses-postop_desc-desikan_meanlength.csv')\\n\",\n",
       " b\"Function: shutil.copyfile('T1.json', '/output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-preproc_T1w.json')\\n\",\n",
       " b\"Function: shutil.copyfile('connectome.csv', '/output/sub-CON03/ses-postop/connectome/sub-CON03_ses-postop_desc-desikan_level-participant_connectome.csv')\\n\",\n",
       " b\"Function: shutil.copyfile('response_csf.txt', '/output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_tissue-CSF_response.txt')\\n\",\n",
       " b'Command:  mrconvert FOD_CSF.mif /output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_tissue-CSF_ODF.nii.gz -strides +1,+2,+3,+4 -clear_property comments\\n',\n",
       " b'Command:  mrconvert FOD_WM.mif /output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_tissue-WM_ODF.nii.gz -strides +1,+2,+3,+4 -clear_property comments\\n',\n",
       " b\"Function: shutil.copyfile('5TT.json', '/output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-5tt_probseg.json')\\n\",\n",
       " b'Command:  mrconvert 5TT.mif /output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-5tt_probseg.nii.gz -strides +1,+2,+3,+4 -clear_property comments\\n',\n",
       " b\"Function: shutil.copyfile('response_gm.txt', '/output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_tissue-GM_response.txt')\\n\",\n",
       " b'Command:  mrconvert vis.mif /output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-vis_probseg.nii.gz -strides +1,+2,+3 -clear_property comments\\n',\n",
       " b\"Function: shutil.copyfile('response_wm.txt', '/output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_tissue-WM_response.txt')\\n\",\n",
       " b\"Function: shutil.copyfile('mu.txt', '/output/sub-CON03/ses-postop/tractogram/sub-CON03_ses-postop_mu.txt')\\n\",\n",
       " b'Command:  mrconvert T1_mask.mif /output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-brain_mask.nii.gz -strides +1,+2,+3 -datatype uint8 -clear_property comments\\n',\n",
       " b'Command:  mrconvert parc.mif /output/sub-CON03/ses-postop/anat/sub-CON03_ses-postop_desc-desikan_dseg.nii.gz -strides +1,+2,+3 -clear_property comments\\n',\n",
       " b'Command:  mrconvert tissues.mif /output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_tissue-all_probseg.nii.gz -strides +1,+2,+3,+4 -clear_property comments\\n',\n",
       " b'Command:  mrconvert FOD_GM.mif /output/sub-CON03/ses-postop/dwi/sub-CON03_ses-postop_tissue-GM_ODF.nii.gz -strides +1,+2,+3,+4 -clear_property comments\\n',\n",
       " b'mrtrix3_connectome.py: Deleting scratch directory /mrtrix3_connectome.py-tmp-593LAU/\\n',\n",
       " b' \\n',\n",
       " b'Batch Job Summary Report for Job \"job_script2\" (25212383) on daint\\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'             Submit            Eligible               Start                 End    Elapsed  Timelimit \\n',\n",
       " b'------------------- ------------------- ------------------- ------------------- ---------- ---------- \\n',\n",
       " b'2020-08-24T12:49:18 2020-08-24T12:49:18 2020-08-24T12:49:19 2020-08-24T16:25:32   03:36:13   23:59:00 \\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'Username    Account     Partition   NNodes   Energy\\n',\n",
       " b'----------  ----------  ----------  ------  --------------\\n',\n",
       " b'bp000309    ich012      normal           1    1.92M joules\\n',\n",
       " b' \\n',\n",
       " b'This job did not utilize any GPUs\\n',\n",
       " b' \\n',\n",
       " b'----------------------------------------------------------\\n',\n",
       " b'Scratch File System        Files       Quota\\n',\n",
       " b'--------------------  ----------  ----------\\n',\n",
       " b'/scratch/snx3000             370     1000000\\n',\n",
       " b' \\n']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_output = mrtrix_job2.working_dir.stat(\"slurm-\" + job_script + \".out\").raw().readlines()\n",
    "slurm_output[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the result\n",
    "\n",
    "It took 36 cores 03:36:54 hrs to run the second part of the tractography pipeline, but that number varies in dependence of many factors, like the used data set and the number of tracks produced.\n",
    "\n",
    "Next, we will build a path to the folder where MRtrix stored the resulting connectome in tvb-processing-sc step 2 and have a look inside the folder.\n",
    "\n",
    "The path to the connectome folder depends on the BIDS sub-id of the subject and -- if used -- the BIDS ses-id of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to MRtrix output connectome\n",
    "connectome_folder = \"tvb_converter_workdir/mrtrix_output/sub-\" + participant_label + \"/ses-\" + session_label + \"/connectome\"\n",
    "SC_folder = wd_handle.stat(connectome_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the typical `recon-all` output folder schema. Nice!  \n",
    "\n",
    "## 10. Upload FreeSurfer license file\n",
    "\n",
    "What we need to do now, in order to finally preprocess the fMRI data, is to upload a FreeSurfer license file. Many will already obtained a FreeSurfer license while downloading/installing FreeSurfer. It is usually located in the FreeSurfer main folder and called  For more information on how to obtain a FreeSurfer license file, see:  \n",
    "\n",
    "https://surfer.nmr.mgh.harvard.edu/fswiki/License\n",
    "\n",
    "Note: the mrtrix3_connectome container already contains a FreeSurfer license, but it is a person-specific license from the container developer, so to be compliant with Usage Terms, please generate and upload your own license file as outlines in the next step.\n",
    "\n",
    "So you generated your license file. Let's upload it. To upload, we perform the same operations as we did to upload the MRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload into base_folder\n",
    "wd_handle.upload(input_name = full_fs_path, destination = 'license.txt')\n",
    "wd_handle.listdir()\n",
    "# create file path\n",
    "fs_license = base_folder + \"license.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run `thevirtualbrain/tvb-pipeline-fmriprep` for fMRI preprocessing\n",
    "\n",
    "Ok, everything is in place, let's create a batch file for `fmriprep`, which performs fMRI preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADJUSTABLE PARAMETERS\n",
    "################################################\n",
    "job_script_fmriprep = \"job_script_fmriprep\" # name of the job script file\n",
    "wall_time = \"23:59:00\"\n",
    "aroma_melodic_dimensionality = \"-120\"\n",
    "################################################\n",
    "\n",
    "with open(job_script_fmriprep, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash -l\\n\")  \n",
    "    f.write(\"#SBATCH --time=\" + wall_time + \"\\n\")\n",
    "    f.write(\"#SBATCH --output=slurm-\" + job_script_fmriprep + \".out\\n\")\n",
    "    f.write(\"#SBATCH --nodes=1\\n\")\n",
    "    f.write(\"#SBATCH --ntasks-per-core=1\\n\")    \n",
    "    f.write(\"#SBATCH --ntasks-per-node=1\\n\")\n",
    "    f.write(\"#SBATCH --cpus-per-task=\" + n_cpus + \"\\n\")\n",
    "    f.write(\"#SBATCH --partition=normal\\n\")\n",
    "    f.write(\"#SBATCH --constraint=mc\\n\")\n",
    "    f.write(\"#SBATCH --hint=nomultithread\\n\") # disable hyperthreading such that all cores become available for multithreading\n",
    "    f.write(\"export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\\n\")\n",
    "    f.write(\"module load /apps/daint/UES/easybuild/modulefiles/daint-mc\\n\")\n",
    "    f.write(\"module load /apps/daint/system/modulefiles/sarus/1.1.0\\n\\n\")\n",
    "    f.write(\"sarus pull thevirtualbrain/tvb-pipeline-fmriprep\\n\")\n",
    "    f.write(\"srun sarus run \" + \n",
    "            \"--mount=type=bind,source=$HOME,destination=$HOME \" +\n",
    "            \"--mount=type=bind,source=\" + input_dir + \",destination=/dataset \" +\n",
    "            \"--mount=type=bind,source=\" + fmriprep_output + \",destination=/fmriprep_out/ \" +\n",
    "            \"--mount=type=bind,source=\" + fmriprep_workdir + \",destination=/fmriprep_workdir/ \" +\n",
    "            \"--mount=type=bind,source=\" + fs_license + \",destination=/license.txt \" +\n",
    "            \"thevirtualbrain/tvb-pipeline-fmriprep \" +\n",
    "            \"/dataset /fmriprep_out/ participant \" +\n",
    "            \"--use-aroma --bold2t1w-dof 6 --nthreads $SLURM_CPUS_PER_TASK \" +\n",
    "            \"--omp-nthreads $SLURM_CPUS_PER_TASK \" +\n",
    "            \"--output-spaces T1w MNI152NLin6Asym:res-2 fsaverage5 \" +\n",
    "            \"--participant_label \" + participant_label + \" \" +\n",
    "            \"--fs-license-file /license.txt \" +\n",
    "            \"--aroma-melodic-dimensionality \" + aroma_melodic_dimensionality + \" \" +\n",
    "            \"-w /fmriprep_workdir\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether it looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash -l\r\n",
      "#SBATCH --time=23:59:00\r\n",
      "#SBATCH --output=slurm-job_script_fmriprep.out\r\n",
      "#SBATCH --nodes=1\r\n",
      "#SBATCH --ntasks-per-core=1\r\n",
      "#SBATCH --ntasks-per-node=1\r\n",
      "#SBATCH --cpus-per-task=36\r\n",
      "#SBATCH --partition=normal\r\n",
      "#SBATCH --constraint=mc\r\n",
      "#SBATCH --hint=nomultithread\r\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\r\n",
      "module load /apps/daint/UES/easybuild/modulefiles/daint-mc\r\n",
      "module load /apps/daint/system/modulefiles/sarus/1.1.0\r\n",
      "\r\n",
      "sarus pull thevirtualbrain/tvb-pipeline-fmriprep\r\n",
      "srun sarus run --mount=type=bind,source=$HOME,destination=$HOME --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/dataset,destination=/dataset --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output,destination=/fmriprep_out/ --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output/tmp,destination=/fmriprep_workdir/ --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/license.txt,destination=/license.txt thevirtualbrain/tvb-pipeline-fmriprep /dataset /fmriprep_out/ participant --use-aroma --bold2t1w-dof 6 --nthreads $SLURM_CPUS_PER_TASK --omp-nthreads $SLURM_CPUS_PER_TASK --output-spaces T1w MNI152NLin6Asym:res-2 fsaverage5 --participant_label CON03 --fs-license-file /license.txt --aroma-melodic-dimensionality -120 -w /fmriprep_workdir\r\n"
     ]
    }
   ],
   "source": [
    "!cat job_script_fmriprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'license.txt': PathFile: license.txt,\n",
       " 'job_script': PathFile: job_script,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'job_script_fmriprep': PathFile: job_script_fmriprep,\n",
       " 'stdout': PathFile: stdout,\n",
       " 'job_script2': PathFile: job_script2,\n",
       " '__MACOSX/': PathDir: __MACOSX/,\n",
       " 'tvb_converter_workdir/': PathDir: tvb_converter_workdir/}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.upload(input_name=job_script_fmriprep, destination = job_script_fmriprep)\n",
    "wd_handle.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `job_script_fmriprep` is there, upload worked. Time to submit the batch job to the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'/scratch/snx3000/unicore/FILESPACE/eccd54c4-0449-41e7-aac1-99f47aa31f61/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmriprep_job = site_client.execute(\"sbatch \" + base_folder + job_script_fmriprep)\n",
    "wd_fmriprep = fmriprep_job.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_fmriprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me just update the table to give us a better overview over handles/folders.\n",
    "\n",
    "Handle | Path variable | Description\n",
    ":---: | :---: | :---:\n",
    "`wd_handle` | `base_folder` | BIDS input, pipeline output\n",
    "`mrtrix_job` | `wd_mrtrix_res` | SLURM job meta output SC 1st part\n",
    "`mrtrix_job2` | `wd_mrtrix_res2` | SLURM job meta output SC 2nd part\n",
    "`fmriprep_job` | `wd_fmriprep` | SLURM job meta output fmriprep\n",
    "\n",
    "\n",
    "Now let's check whether fmriprep is (still) running or finished.\n",
    "\n",
    "# Please note: There seems to be a strange bug in Sarus: sometimes the command   \n",
    "\n",
    "# `srun sarus pull thevirtualbrain/tvb-pipeline-fmriprep`   \n",
    "\n",
    "# works and sometimes not. It's often easier to just log into your supercomputing account via SSH and run the pull command in the shell. Once the container with the \"latest\" tag was pulled it doesn't need to be re-pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'200824-23:58:56,436 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Running \"ica_aroma_confound_extraction\" (\"fmriprep.interfaces.confounds.ICAConfounds\")\\n',\n",
       " b'200824-23:58:56,446 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Finished \"fmriprep_wf.single_subject_CON03_wf.func_preproc_ses_postop_task_rest_wf.ica_aroma_wf.add_nonsteady\".\\n',\n",
       " b'200824-23:58:56,492 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Finished \"fmriprep_wf.single_subject_CON03_wf.func_preproc_ses_postop_task_rest_wf.ica_aroma_wf.ica_aroma_confound_extraction\".\\n',\n",
       " b'200824-23:59:24,721 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Setting-up \"fmriprep_wf.single_subject_CON03_wf.func_preproc_ses_postop_task_rest_wf.ica_aroma_wf.ica_aroma_metadata_fmt\" in \"/fmriprep_workdir/fmriprep_wf/single_subject_CON03_wf/func_preproc_ses_postop_task_rest_wf/ica_aroma_wf/ica_aroma_metadata_fmt\".\\n',\n",
       " b'200824-23:59:24,747 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Running \"ica_aroma_metadata_fmt\" (\"niworkflows.interfaces.utils.TSV2JSON\")\\n',\n",
       " b'200824-23:59:24,768 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Finished \"fmriprep_wf.single_subject_CON03_wf.func_preproc_ses_postop_task_rest_wf.ica_aroma_wf.ica_aroma_metadata_fmt\".\\n',\n",
       " b'200824-23:59:24,800 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Setting-up \"fmriprep_wf.single_subject_CON03_wf.func_preproc_ses_postop_task_rest_wf.aroma_confounds\" in \"/fmriprep_workdir/fmriprep_wf/single_subject_CON03_wf/func_preproc_ses_postop_task_rest_wf/aroma_confounds\".\\n',\n",
       " b'200824-23:59:24,893 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Running \"aroma_confounds\" (\"nipype.interfaces.utility.wrappers.Function\")\\n',\n",
       " b'200824-23:59:24,913 nipype.workflow INFO:\\n',\n",
       " b'\\t [Node] Finished \"fmriprep_wf.single_subject_CON03_wf.func_preproc_ses_postop_task_rest_wf.aroma_confounds\".\\n',\n",
       " b'200824-23:59:32,972 nipype.workflow IMPORTANT:\\n',\n",
       " b'\\t fMRIPrep finished successfully!\\n',\n",
       " b'200824-23:59:32,978 nipype.workflow IMPORTANT:\\n',\n",
       " b'\\t Works derived from this fMRIPrep execution should include the boilerplate text found in <OUTPUT_PATH>/fmriprep/logs/CITATION.md.\\n',\n",
       " b' \\n',\n",
       " b'Batch Job Summary Report for Job \"job_script_fmriprep\" (25219730) on daint\\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'             Submit            Eligible               Start                 End    Elapsed  Timelimit \\n',\n",
       " b'------------------- ------------------- ------------------- ------------------- ---------- ---------- \\n',\n",
       " b'2020-08-24T18:25:27 2020-08-24T18:25:27 2020-08-24T18:25:30 2020-08-24T23:59:43   05:34:13   23:59:00 \\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'Username    Account     Partition   NNodes   Energy\\n',\n",
       " b'----------  ----------  ----------  ------  --------------\\n',\n",
       " b'bp000309    ich012      normal           1    2.53M joules\\n',\n",
       " b' \\n',\n",
       " b'This job did not utilize any GPUs\\n',\n",
       " b' \\n',\n",
       " b'----------------------------------------------------------\\n',\n",
       " b'Scratch File System        Files       Quota\\n',\n",
       " b'--------------------  ----------  ----------\\n',\n",
       " b'/scratch/snx3000            5723     1000000\\n',\n",
       " b' \\n']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_output = fmriprep_job.working_dir.stat(\"slurm-\" + job_script_fmriprep + \".out\").raw().readlines()\n",
    "slurm_output[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know the drill: unless there is the job summary at the end, the job hasn't finished.\n",
    "\n",
    "## 12. Select `recon-all` output\n",
    "\n",
    "Either of the two Apps, `mrtrix3_connectome` and `fmriprep`, may have run `recon-all` (depending on which parcellation you have chosen). Here, we set the appropriate folder. In this example we used the parcellation `desikan` and `mrtrix3_connectome` generated `recon_all` output which we re-used later in `fmriprep`. So let's configure accordingly. Below are the two alternatives in different cells, choose only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output/freesurfer\n",
      "sub-CON03\n"
     ]
    }
   ],
   "source": [
    "# Option 1: recon_all run by fmriprep\n",
    "recon_all_dir = base_folder + \"tvb_converter_workdir/fmriprep_output/freesurfer\"\n",
    "recon_all_subject_name = \"sub-\" + participant_label\n",
    "print(recon_all_dir)\n",
    "print(recon_all_subject_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freesurfer_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-56-f475580857ff>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Option 2: recon_all run by MRtrix3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mrecon_all_dir\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbase_folder\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mfreesurfer_path\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m# cut-off the last \"freesurfer\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mrecon_all_subject_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"freesurfer\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'freesurfer_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Option 2: recon_all run by MRtrix3\n",
    "recon_all_dir = base_folder + freesurfer_path[:-10] # cut-off the last \"freesurfer\"\n",
    "recon_all_subject_name = \"freesurfer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output/freesurfer'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_all_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 13. Run BIDS App `tvb_converter` to generate TVB input\n",
    "\n",
    "What remains to be done is to create a batch file for the tvb_converter BIDS App, upload it and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ADJUSTABLE PARAMETERS\n",
    "################################################\n",
    "wall_time = \"01:00:00\"\n",
    "cpu_per_task = \"36\"\n",
    "job_script_tvb_conv = \"job_script_tvb_conv\" # name of the job script file\n",
    "################################################\n",
    "\n",
    "# FIXED PARAMETERS\n",
    "################################################\n",
    "weights_path = \"/mrtrix3_out/sub-\" + participant_label + \"/connectome/sub-\" + participant_label + \"_parc-\" + parcellation + \"_level-participant_connectome.csv\"\n",
    "tracts_path = \"/mrtrix3_out/sub-\" + participant_label + \"/connectome/sub-\" + participant_label + \"_parc-\"+ parcellation+ \"_meanlength.csv\"\n",
    "################################################\n",
    "\n",
    "\n",
    "with open(job_script_tvb_conv, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash -l\\n\")  \n",
    "    f.write(\"#SBATCH --time=\" + wall_time + \"\\n\")\n",
    "    f.write(\"#SBATCH --output=slurm-\" + job_script_tvb_conv + \".out\\n\")\n",
    "    f.write(\"#SBATCH --nodes=1\\n\")\n",
    "    f.write(\"#SBATCH --ntasks-per-core=1\\n\")    \n",
    "    f.write(\"#SBATCH --ntasks-per-node=1\\n\")\n",
    "    f.write(\"#SBATCH --cpus-per-task=\" + cpu_per_task + \"\\n\")\n",
    "    f.write(\"#SBATCH --partition=normal\\n\")\n",
    "    f.write(\"#SBATCH --constraint=mc\\n\")\n",
    "    f.write(\"#SBATCH --hint=nomultithread\\n\") # disable hyperthreading such that all cores become available for multithreading\n",
    "    f.write(\"export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\\n\")\n",
    "    f.write(\"module load /apps/daint/UES/easybuild/modulefiles/daint-mc\\n\")\n",
    "    f.write(\"module load /apps/daint/system/modulefiles/sarus/1.1.0\\n\\n\")\n",
    "    f.write(\"sarus pull thevirtualbrain/tvb-pipeline-converter:1.0\\n\")\n",
    "    f.write(\"srun sarus run \" +\n",
    "            \"--mount=type=bind,source=`dirname \" + fs_license + \"`,destination=/freesurfer_license_dir/ \" + \n",
    "            \"--mount=type=bind,source=\" + input_dir + \",destination=/input_dir \" +\n",
    "            \"--mount=type=bind,source=\" + output_dir + \",destination=/output_dir \" +\n",
    "            \"--mount=type=bind,source=\" + mrtrix_output + \",destination=/mrtrix3_out \" +\n",
    "            \"--mount=type=bind,source=\" + fmriprep_output + \",destination=/fmriprep_out \" +\n",
    "            \"--mount=type=bind,source=\" + fmriprep_workdir + \",destination=/fmriprep_workdir \" +\n",
    "            \"--mount=type=bind,source=\" + tvb_output + \",destination=/tvb_out \" +\n",
    "            \"--mount=type=bind,source=\" + tvb_workdir + \",destination=/tvb_workdir \" +\n",
    "            \"--mount=type=bind,source=\" + recon_all_dir + \",destination=/recon_all_dir \" +\n",
    "            \"--entrypoint thevirtualbrain/tvb-pipeline-converter:1.0 \" +\n",
    "            \"/bin/bash -c \\\"cp /freesurfer_license_dir/license.txt /opt/freesurfer/ && \" +\n",
    "                            \"mkdir -p /recon_all_dir/\" + recon_all_subject_name + \"/bem && \" +\n",
    "                            \"cd /recon_all_dir/\" + recon_all_subject_name + \"/bem && \" +\n",
    "                            \"/tvb_converter_pipeline.sh /input_dir /output_dir /mrtrix3_out /fmriprep_out \" +\n",
    "                            \"/fmriprep_workdir /tvb_out /tvb_workdir /recon_all_dir \" + recon_all_subject_name + \" \" +\n",
    "                            participant_label + \" \" + task_name + \" \" + parcellation + \" \" + weights_path + \n",
    "                            \" \" + tracts_path + \" \" + cpu_per_task + \"\\\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash -l\r\n",
      "#SBATCH --time=01:00:00\r\n",
      "#SBATCH --output=slurm-job_script_tvb_conv.out\r\n",
      "#SBATCH --nodes=1\r\n",
      "#SBATCH --ntasks-per-core=1\r\n",
      "#SBATCH --ntasks-per-node=1\r\n",
      "#SBATCH --cpus-per-task=36\r\n",
      "#SBATCH --partition=normal\r\n",
      "#SBATCH --constraint=mc\r\n",
      "#SBATCH --hint=nomultithread\r\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\r\n",
      "module load /apps/daint/UES/easybuild/modulefiles/daint-mc\r\n",
      "module load /apps/daint/system/modulefiles/sarus/1.1.0\r\n",
      "\r\n",
      "sarus pull thevirtualbrain/tvb-pipeline-converter:1.0\r\n",
      "srun sarus run --mount=type=bind,source=`dirname /scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/license.txt`,destination=/freesurfer_license_dir/ --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/dataset,destination=/input_dir --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir,destination=/output_dir --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/mrtrix_output,destination=/mrtrix3_out --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output,destination=/fmriprep_out --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output/tmp,destination=/fmriprep_workdir --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/TVB_output,destination=/tvb_out --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/TVB_output/tmp,destination=/tvb_workdir --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/01d7d4ef-6a79-4480-9d72-070a2c1975fc/tvb_converter_workdir/fmriprep_output/freesurfer,destination=/recon_all_dir --entrypoint thevirtualbrain/tvb-pipeline-converter:1.0 /bin/bash -c \"cp /freesurfer_license_dir/license.txt /opt/freesurfer/ && mkdir -p /recon_all_dir/sub-CON03/bem && cd /recon_all_dir/sub-CON03/bem && /tvb_converter_pipeline.sh /input_dir /output_dir /mrtrix3_out /fmriprep_out /fmriprep_workdir /tvb_out /tvb_workdir /recon_all_dir sub-CON03 CON03 ArchiSocial desikan /mrtrix3_out/sub-CON03/connectome/sub-CON03_parc-desikan_level-participant_connectome.csv /mrtrix3_out/sub-CON03/connectome/sub-CON03_parc-desikan_meanlength.csv 36\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat job_script_tvb_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Upload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'license.txt': PathFile: license.txt,\n",
       " 'job_script': PathFile: job_script,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'job_script_fmriprep': PathFile: job_script_fmriprep,\n",
       " 'job_script_tvb_conv': PathFile: job_script_tvb_conv,\n",
       " 'stdout': PathFile: stdout,\n",
       " 'job_script2': PathFile: job_script2,\n",
       " '__MACOSX/': PathDir: __MACOSX/,\n",
       " 'tvb_converter_workdir/': PathDir: tvb_converter_workdir/}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.upload(input_name=job_script_tvb_conv, destination = job_script_tvb_conv)\n",
    "wd_handle.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The file is there, upload worked. Time to submit the batch job to the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'/scratch/snx3000/unicore/FILESPACE/fcd13f78-2ca6-4e0d-9908-0825224dcbe5/'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvb_conv_job = site_client.execute(\"sbatch \" + base_folder + job_script_tvb_conv)\n",
    "wd_tvb_conv = tvb_conv_job.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_tvb_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's update the table one last time\n",
    "\n",
    "Handle | Path variable | Description\n",
    ":---: | :---: | :---:\n",
    "`wd_handle` | `base_folder` | BIDS input, pipeline output\n",
    "`mrtrix_job` | `wd_mrtrix_res` | SLURM job meta output MRtrix3\n",
    "`fmriprep_job` | `wd_fmriprep` | SLURM job meta output fmriprep\n",
    "`tvb_conv_job` | `wd_tvb_conv` | SLURM job meta output tvb_converter\n",
    "\n",
    "\n",
    "Let's check whether tvb_converter is (still) running or finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'\\n',\n",
       " b'\\n',\n",
       " b'Image Exception : #22 :: ERROR: Could not open image /fmriprep_out/fmriprep/sub-CON03/func/sub-CON03_task-ArchiSocial_space-T1w_boldref\\n',\n",
       " b\"terminate called after throwing an instance of 'RBD_COMMON::BaseException'\\n\",\n",
       " b'/tvb_converter_pipeline.sh: line 40:   592 Aborted                 (core dumped) flirt -in ${mrtrix_output}/sub-${participant_label}/anat/sub-${participant_label}_parc-${parcellation}_indices.nii.gz -out ${mrtrix_output}/sub-${participant_label}/anat/sub-${participant_label}_parc-${parcellation}_indices_resample2bold.nii.gz -ref ${fmriprep_output}/fmriprep/sub-${participant_label}/func/sub-${participant_label}_task-${task_name}_space-T1w_boldref.nii.gz -nosearch -interp nearestneighbour\\n',\n",
       " b'Extract fmri ROI timeseries\\n',\n",
       " b'Image Exception : #22 :: ERROR: Could not open image /fmriprep_out/fmriprep/sub-CON03/func/sub-CON03_task-ArchiSocial_space-T1w_AromaNonAggressiveDenoised\\n',\n",
       " b\"terminate called after throwing an instance of 'RBD_COMMON::BaseException'\\n\",\n",
       " b'/tvb_converter_pipeline.sh: line 47:   593 Aborted                 (core dumped) fslmeants -o ${fMRI_ROI_ts} -i ${fmriprep_output}/fmriprep/sub-${participant_label}/func/sub-${participant_label}_task-${task_name}_space-T1w_AromaNonAggressiveDenoised.nii.gz --label=${mrtrix_output}/sub-${participant_label}/anat/sub-${participant_label}_parc-${parcellation}_indices_resample2bold.nii.gz\\n',\n",
       " b'Run the TVB converter py script.\\n',\n",
       " b'Traceback (most recent call last):\\n',\n",
       " b'  File \"/convert2TVB_format.py\", line 79, in <module>\\n',\n",
       " b'    mrtrix_lut    = np.genfromtxt(mrtrix_output_dir+\"/parc-desikan_lookup.txt\", skip_header=4, dtype=\"str\")\\n',\n",
       " b'  File \"/opt/conda/envs/mne/lib/python3.7/site-packages/numpy/lib/npyio.py\", line 1744, in genfromtxt\\n',\n",
       " b\"    fhd = iter(np.lib._datasource.open(fname, 'rt', encoding=encoding))\\n\",\n",
       " b'  File \"/opt/conda/envs/mne/lib/python3.7/site-packages/numpy/lib/_datasource.py\", line 266, in open\\n',\n",
       " b'    return ds.open(path, mode, encoding=encoding, newline=newline)\\n',\n",
       " b'  File \"/opt/conda/envs/mne/lib/python3.7/site-packages/numpy/lib/_datasource.py\", line 624, in open\\n',\n",
       " b'    raise IOError(\"%s not found.\" % path)\\n',\n",
       " b'OSError: /mrtrix3_out/parc-desikan_lookup.txt not found.\\n',\n",
       " b'srun: error: nid00057: task 0: Exited with exit code 1\\n',\n",
       " b'srun: Terminating job step 25235760.0\\n',\n",
       " b' \\n',\n",
       " b'Batch Job Summary Report for Job \"job_script_tvb_conv\" (25235760) on daint\\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'             Submit            Eligible               Start                 End    Elapsed  Timelimit \\n',\n",
       " b'------------------- ------------------- ------------------- ------------------- ---------- ---------- \\n',\n",
       " b'2020-08-25T10:02:13 2020-08-25T10:02:13 2020-08-25T10:02:15 2020-08-25T10:04:31   00:02:16   01:00:00 \\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'Username    Account     Partition   NNodes   Energy\\n',\n",
       " b'----------  ----------  ----------  ------  --------------\\n',\n",
       " b'bp000309    ich012      normal           1   19.76K joules\\n',\n",
       " b' \\n',\n",
       " b'This job did not utilize any GPUs\\n',\n",
       " b' \\n',\n",
       " b'----------------------------------------------------------\\n',\n",
       " b'Scratch File System        Files       Quota\\n',\n",
       " b'--------------------  ----------  ----------\\n',\n",
       " b'/scratch/snx3000            6011     1000000\\n',\n",
       " b' \\n']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_output = tvb_conv_job.working_dir.stat(\"slurm-\" + job_script_tvb_conv + \".out\").raw().readlines()\n",
    "slurm_output[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 14. Download TVB ready data\n",
    "\n",
    " On the supercomputer, results are stored in the folder\n",
    "```\n",
    "<tvb_output>\n",
    "```\n",
    "\n",
    "If everything went smoothly, there will be nine files in this folder:  \n",
    "\n",
    "```\n",
    "sub-<participant_label>_task-rest_parc-desikan_ROI_timeseries.txt\n",
    "sub-<participant_label>_EEGProjection.mat\n",
    "sub-<participant_label>_region_mapping.txt\n",
    "sub-<participant_label>_inner_skull_surface.zip\n",
    "sub-<participant_label>_Cortex.zip\n",
    "sub-<participant_label>_outer_skull_surface.zip\n",
    "sub-<participant_label>_outer_skin_surface.zip\n",
    "sub-<participant_label>_EEG_Locations.txt\n",
    "sub-<participant_label>_Connectome.zip\n",
    "```\n",
    "\n",
    "We will go on to zip the folder and download the results to the Docker image filesystem in which the Jupyter notebook client is running. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'license.txt': PathFile: license.txt,\n",
       " 'job_script': PathFile: job_script,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'job_script_fmriprep': PathFile: job_script_fmriprep,\n",
       " 'job_script_tvb_conv': PathFile: job_script_tvb_conv,\n",
       " 'stdout': PathFile: stdout,\n",
       " 'job_script2': PathFile: job_script2,\n",
       " '__MACOSX/': PathDir: __MACOSX/,\n",
       " 'tvb_converter_workdir/': PathDir: tvb_converter_workdir/}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "site_client.execute(\"zip -r \" + tvb_output+\".zip \" + tvb_output)\n",
    "output_hdl = wd_handle.stat(\"tvb_converter_workdir/TVB_output.zip\")\n",
    "output_hdl.download(\"TVB_output.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left to do is to move the TVB_output.zip file to the persisted-data/data folder if you want this archive to be persisted or you can download it by checking the checkbox and hitting the \"Download\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Additionally the created derivatives for TVB are stored in BIDS format inside the dataset provided at the beginning.\n",
    "We will now zip and download this one too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Download pipeline output in BIDS format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline results are also stored according to the BIDS specifications inside the /derivatives/TVB directory of the given BIDS folder. \n",
    "The data are stored following the specifications of the Common derivatives and Computational models BIDS extension proposal. <br><br>\n",
    "\n",
    "Common derivatives <br>\n",
    "https://docs.google.com/document/d/1Wwc4A6Mow4ZPPszDIWfCUCRNstn7d_zzaWPcfcHmgI4/edit <br>\n",
    "<br>\n",
    "Computational models <br>\n",
    "https://docs.google.com/document/d/1oaBWmkrUqH28oQb1PTO-rG_kuwNX9KqAoE9i5iDh1xw/edit#heading=h.mqkmyp254xh6 <br>\n",
    "<br>\n",
    "\n",
    "What remains is to zip, download the BIDS folder from the HPC and to add a additional dataset_description.json to the /derivatives directory to comply with BIDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# zip the BIDS folder on the HPC\n",
    "site_client.execute(\"cd \" + base_folder + \" && zip -r \" + dataset + \" \" \n",
    "                    + dataset.strip(\".zip\"))\n",
    "\n",
    "# download the BIDS folder\n",
    "output_hdl = wd_handle.stat(dataset)\n",
    "output_hdl.download(dataset)\n",
    "\n",
    "# unzip \n",
    "import zipfile\n",
    "with zipfile.ZipFile(dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate BIDS-compliant metadata file dataset_description.json\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Arguments\n",
    "input_dataset_description_json = dataset.strip(\".zip\")+'/dataset_description.json'\n",
    "output_dataset_description_json = dataset.strip(\".zip\")+'/dataset_description.json'\n",
    "\n",
    "\n",
    "# 1 read the dataset_description.json from the raw data set \n",
    "with open(input_dataset_description_json, \"r\") as input_json:\n",
    "    data = json.load(input_json)\n",
    "\n",
    "\n",
    "# 2 prepend \"TVB pipeline derivative: \" to the value of the key \"Name\" \n",
    "# to indicate that this is not the original raw data set, but\n",
    "# a derivative generated by the TVB pipeline.\n",
    "data['Name'] = \"TVB pipeline derivative: \" + data['Name'] \n",
    "\n",
    "\n",
    "## Uncomment below to generate the data dict from scratch\n",
    "#data= OrderedDict()\n",
    "#data['Name'] = ''\n",
    "#data['BIDSVersion'] = '1.0.2'\n",
    "#data['License'] = ''\n",
    "#data['Authors'] = ['','','']\n",
    "#data['HowToAcknowledge'] = ''\n",
    "#data['Funding'] = ['','','']\n",
    "#data['ReferencesAndLinks'] = ['','','']\n",
    "#data['DatasetDOI'] = ''\n",
    "\n",
    "\n",
    "# 3 Add fields for BIDS derivatives\n",
    "data['PipelineDescription'] = {\n",
    "    \t\"Name\": \"TVB\", # REQUIRED: this field must be a substring of the folder name of the pipeline output\n",
    "        \"Version\": \"1.0\", \n",
    "    \t\"CodeURL\": \"https://github.com/BrainModes\",\n",
    "    \t\"DockerHubContainerTag\": \"thevirtualbrain/tvb_pipeline\",\n",
    "    \t\"SingularityContainerURL\": \"\",\n",
    "    \t\"SingularityContainerVersion\": \"\" \n",
    "    }\n",
    "\n",
    "data['SourceDatasets'] = [\n",
    "    \t{\n",
    "    \t\t\"URL\": \"\",\n",
    "    \t\t\"DOI\": data['DatasetDOI'],\n",
    "    \t\t\"Version\": \"\"\n",
    "    \t}\n",
    "#    \t,{\n",
    "#    \t\t\"URL\": \"\",\n",
    "#    \t\t\"DOI\": \"\",\n",
    "#    \t\t\"Version\": \"\"\n",
    "#    \t}    \n",
    "    ]\n",
    "\n",
    "\n",
    "# 4 Save JSON\n",
    "with open(output_dataset_description_json, 'w') as ff:\n",
    "    json.dump(data, ff,sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ls -l dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/BIDS_test.zip'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip the BIDS folder\n",
    "shutil.make_archive(dataset.strip(\".zip\"), 'zip', dataset.strip(\".zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'521f92bb-1c6c-4dd9-907d-8a225ca914ae'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and download to the collab storage\n",
    "collab_storage.upload_file(dataset, COLLAB_PATH+\"/BIDS_TVB_derivatives.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with HBP Knowledgegraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate the produced pipeline output into KnowledgeGraph, the user is required to provide values for MINDS metadata keys. The provided metadata is stored in the file minds_metadata.json in order to be read out by the curation team. <br>\n",
    "<br>\n",
    "Please read through the following lines of code and enter applicable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate MINDS-compliant metadata file minds_metadata.json\n",
    "\n",
    "# Arguments\n",
    "output_MINDS_JSON = 'minds_metadata.json'\n",
    "\n",
    "# Pipeline users are requested to fill out the following fields\n",
    "# in order to integrate data with KnowledgeGraph\n",
    "# Enter the information between the \"\"\n",
    "Dataset__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "Dataset__has_contributor = \"\" # enter corresponding Person block _ID \n",
    "Dataset_created_as = \"\" # choose: HBP-SGA1, HBP-SGA2, HBP-SGA3, external \n",
    "Dataset__has_custodian = \"\" # enter corresponding Person block _ID \n",
    "Dataset_description = \"\" # describe the content of this metadata block \n",
    "Dataset_DOI = \"\" # if this dataset already has a DOI, please enter it here (otherwise a DOI will be assigned to this dataset when it is released \n",
    "Dataset_embargo_status = \"\" # choose: embargoed, not-embargoed \n",
    "Dataset_intended_release_date = \"\" # if you chose \"embargoed\" please state here the intended release date (format: yyyy-mm-dd) \n",
    "Dataset_license = \"\" # enter the data license, choose (e.g.) from Creative Comments licence list \n",
    "Dataset__has_main_contact = \"\" # enter corresponding Person block _ID \n",
    "Dataset__has_main_file_bundle = \"\" # enter the corresponding FileBundle block _ID \n",
    "Dataset_title = \"\" # enter a meaningful title for this metadata block \n",
    "EthicsApproval__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "EthicsApproval_authority = \"\" # enter the ethics authority name of this ethics approval \n",
    "EthicsApproval_country = \"\" # enter the country of this ethics approval was issued \n",
    "EthicsApproval_ID = \"\" # enter the received identifier of this ethics approval \n",
    "EthicsApproval_SP12_approval = \"\" # choose: yes, no \n",
    "File__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "File_description = \"\" # describe the content of this metadata block \n",
    "File_format = \"\" # enter format of file of this File block \n",
    "File_title = \"\" # enter a meaningful title for this metadata block \n",
    "File_URL = \"\" # enter URL to file of this File block \n",
    "FileBundle__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "FileBundle_description = \"\" # describe the content of this metadata block \n",
    "FileBundle_format = \"\" # enter format of file occurring in this FileBundle block \n",
    "FileBundle_tag = \"\" # enter tag for grouping reason \n",
    "FileBundle_title = \"\" # enter a meaningful title for this metadata block \n",
    "FileBundle_URL = \"\" # enter URL to the file-folder or URLs to corresponding files belonging to this FileBundle block (multiple entries as multiple rows) \n",
    "FundingInformation__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "FundingInformation_grant_ID = \"\" # enter identification number of the grant of this FundingInformation block  \n",
    "FundingInformation_name = \"\" # enter name of funding institution of this FundingInformation block  \n",
    "MethodParadigm__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "MethodParadigm_abbreviation = \"\" # enter abbreviated name of instance described in this metadata block \n",
    "MethodParadigm_description = \"\" # describe the content of this metadata block \n",
    "MethodParadigm_experimental_type = \"\" # choose: in vivo, ex vivo, in utero, in vitro, in situ, in silico \n",
    "MethodParadigm_full_name = \"\" # enter full name of instance described in this metadata block \n",
    "MethodParadigm_type = \"\" # enter high level type to which the method/paradigm of this Method/Paradigm block belongs to (e.g., imaging, behavioral assay) \n",
    "ModelInstance__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "ModelInstance__is_modelling_sub_cellular_target = \"\" # enter corresponding StudyTarget block _ID \n",
    "ModelInstance_abstraction_level = \"\" # choose: protein structure, systems biology, spiking neurons, rate neurons, population modelling, cognitive modelling or enter other abstraction level \n",
    "ModelInstance_alias = \"\" # enter alternative name for this metadata block  \n",
    "ModelInstance__is_modelling_brain_structure = \"\" # enter corresponding StudyTarget block _ID \n",
    "ModelInstance__has_contributor = \"\" # enter corresponding Person block _ID \n",
    "ModelInstance__has_custodian = \"\" # enter corresponding Person block _ID \n",
    "ModelInstance_description = \"\" # describe content of this metadata block \n",
    "ModelInstance__has_main_contact = \"\" # enter corresponding Person block _ID \n",
    "ModelInstance_model_format = \"\" # choose: NeuroML, PyNN, SONATA, NEURON-Python, NEURON-Hoc, NEST-SLI, NEST-PYTHON, Java, C++, C, Brian, NineML, MATLAB, NetPyNE \n",
    "ModelInstance_model_format_version_compatibility = \"\" # enter which model format version is compatible for the model of this ModelInstant block \n",
    "ModelInstance_model_scope = \"\" # choose: subcellular model (spine, ion channel, signalling, or molecular), single cell model, network model (microcircuit, brain region, or whole brain) or enter other model scope \n",
    "ModelInstance_species = \"\" # enter binomial name of species used for this ModelInstant (e.g., Homo sapiens, Mus musculus, Rattus norvegicus, Macaca mulatta, Macaca fascicularis) \n",
    "ModelInstance_title = \"\" # enter a meaningful title for this metadata block \n",
    "ModelInstance_version = \"\" # enter version number for model described in this ModelInstance \n",
    "Person__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "Person_email = \"\" # if no ORCID, please enter email of this Person \n",
    "Person_first_name = \"\" # if no ORCID, please enter first name of this Person \n",
    "Person_last_name = \"\" # if no ORCID, please enter last name of this Person \n",
    "Person_ORCID = \"\" # if available, enter ORCID of this Person \n",
    "PLAComponent__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "PLAComponent_associated_task_ID = \"\" # enter identifier of associated HBP task for corresponding PLA component \n",
    "PLAComponent_ID = \"\" # enter identifier of corresponding PLA component \n",
    "PLAComponent__has_owner = \"\" # enter corresponding Person block _ID \n",
    "PLAComponent_phase = \"\" # enter HBP project phase of corresponding PLA component (e.g., HBP-SGA1) \n",
    "Project__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "Project__has_coordinator = \"\" # enter corresponding Person block_ID \n",
    "Project_description = \"\" # describe the content of this metadata block \n",
    "Project_title = \"\" # enter a meaningful title for this metadata block \n",
    "PublicationResource__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "PublicationResource_ID = \"\" # enter identifier of corresponding publication/resource \n",
    "PublicationResource_ID_type = \"\" # choose: DOI, ISSN, ISBN, URL, or other identifier type \n",
    "StudyTarget__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "StudyTarget_abbreviation = \"\" # enter abbreviated name of instance described in this metadata block \n",
    "StudyTarget_full_name = \"\" # enter full name of instance described in this metadata block \n",
    "StudyTarget_source_of_name = \"\" # if full name is chosen from a terminology or ontology, enter name of corresponding terminology or ontology list \n",
    "StudyTarget_type = \"\" # choose: disease, disease model, tissue type, brain structure, cell type, subcellular structure, macromolecular complex, biological process \n",
    "Subject__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "Subject_age = \"\" # enter age of subject described in this Subject block \n",
    "Subject_age_category = \"\" # choose: embryo, neonate, juvenile, young adult, adult, aged adult \n",
    "Subject_age_range_max = \"\" # within in the time frame of the experiment, enter maximum age of subject described in this Subject block \n",
    "Subject_age_range_min = \"\" # within in the time frame of the experiment, enter minimum age of subject described in this Subject block \n",
    "Subject_alias = \"\" # enter alternative name for this metadata block  \n",
    "Subject_disabilitydisease = \"\" # enter disability, disease or disease model the subject of this Subject block has  \n",
    "Subject_genotype = \"\" # enter genotype of the subject described in this Subject block \n",
    "Subject_handedness = \"\" # choose: left, right, ambidextrous \n",
    "Subject_sex = \"\" # choose: female, male, hermaphrodite \n",
    "Subject_species = \"\" # enter binomial species name of this Subject (e.g., Homo sapiens, Mus musculus, Rattus norvegicus, Macaca mulatta, Macaca fascicularis) \n",
    "Subject_strain = \"\" # enter strain of the subject described in this Subject block \n",
    "SubjectGroup__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "SubjectGroup_age_category = \"\" # choose: embryo, neonate, juvenile, young adult, adult, aged adult \n",
    "SubjectGroup_age_range_max = \"\" # within in the time frame of the experiment, enter maximum age of subject described in this Subject block \n",
    "SubjectGroup_age_range_min = \"\" # within in the time frame of the experiment, enter minimum age of subject described in this Subject block \n",
    "SubjectGroup_alias = \"\" # enter alternative name for this metadata block  \n",
    "SubjectGroup_description = \"\" # describe content of this metadata block \n",
    "SubjectGroup_disabilitydisease = \"\" # enter disability, disease or disease model the subjects connected to this SubjectGroup block have  \n",
    "SubjectGroup_genotype = \"\" # enter genotype of subjects connected to this SubjectGroup block \n",
    "SubjectGroup_handedness = \"\" # choose: left, right, ambidextrous \n",
    "SubjectGroup_number_of_subjects = \"\" # enter number of subjects connected to this SubjectGroup block\n",
    "SubjectGroup_sex = \"\" # choose: female, male, hermaphrodite \n",
    "SubjectGroup_species = \"\" # enter binomial name of species occurring in this SubjectGroup (e.g., Homo sapiens, Mus musculus, Rattus norvegicus, Macaca mulatta, Macaca fascicularis) \n",
    "SubjectGroup_strain = \"\" # enter strain of subjects connected to this SubjectGroup block \n",
    "TissueSample__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "TissueSample_alias = \"\" # enter alternative name for this metadata block  \n",
    "TissueSample_hemisphere = \"\" # choose: left, right \n",
    "TissueSample_pathology = \"\" # enter pathology state of tissue described in this TissueSample block \n",
    "TissueSample_type = \"\" # choose: whole brain, hemisphere, slice, brain part, cortical column, or enter other tissue sample type \n",
    "\n",
    "##################################################\n",
    "# Create MINDS JSON object\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "data = OrderedDict()\n",
    "\n",
    "data['Dataset'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": Dataset__ID,\n",
    "    \t\t\"_has_contributor\": Dataset__has_contributor,\n",
    "    \t\t\"created as\": Dataset_created_as,\n",
    "            \"_has_custodian\": Dataset__has_custodian,\n",
    "    \t\t\"description\": Dataset_description,\n",
    "    \t\t\"DOI\": Dataset_DOI,\n",
    "            \"embargo status\": Dataset_embargo_status,\n",
    "    \t\t\"intended release date\": Dataset_intended_release_date,\n",
    "    \t\t\"license\": Dataset_license,\n",
    "            \"_has_main_contact\": Dataset__has_main_contact,\n",
    "    \t\t\"_has_main_file_bundle\": Dataset__has_main_file_bundle,\n",
    "    \t\t\"title\": Dataset_title\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['EthicsApproval'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": EthicsApproval__ID,\n",
    "    \t\t\"authority\": EthicsApproval_authority,\n",
    "    \t\t\"country\": EthicsApproval_country,\n",
    "            \"ID\": EthicsApproval_ID,\n",
    "    \t\t\"SP12 approval\": EthicsApproval_SP12_approval\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['File'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": File__ID,\n",
    "    \t\t\"description\": File_description,\n",
    "    \t\t\"format\": File_format,\n",
    "            \"title\": File_title,\n",
    "    \t\t\"URL\": File_URL\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['FileBundle'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": FileBundle__ID,\n",
    "    \t\t\"description\": FileBundle_description,\n",
    "    \t\t\"format\": FileBundle_format,\n",
    "            \"tag\": FileBundle_tag,\n",
    "    \t\t\"title\": FileBundle_title,\n",
    "    \t\t\"URL\": FileBundle_URL\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['FundingInformation'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": FundingInformation__ID,\n",
    "    \t\t\"grant ID\": FundingInformation_grant_ID,\n",
    "    \t\t\"name\": FundingInformation_name\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['Method/Paradigm'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": MethodParadigm__ID,\n",
    "    \t\t\"abbreviation\": MethodParadigm_abbreviation,\n",
    "    \t\t\"description\": MethodParadigm_description,\n",
    "            \"experimental type\": MethodParadigm_experimental_type,\n",
    "    \t\t\"full name\": MethodParadigm_full_name,\n",
    "            \"type\": MethodParadigm_type\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['ModelInstance'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": ModelInstance__ID,\n",
    "    \t\t\"_is_modelling_sub_cellular_target\": ModelInstance__is_modelling_sub_cellular_target,\n",
    "    \t\t\"abstraction level\": ModelInstance_abstraction_level,\n",
    "    \t\t\"alias\": ModelInstance_alias,\n",
    "            \"_is_modelling_brain_structure\": ModelInstance__is_modelling_brain_structure,\n",
    "    \t\t\"_has_contributor\": ModelInstance__has_contributor,\n",
    "            \"_has_custodian\": ModelInstance__has_custodian,\n",
    "            \"description\": ModelInstance_description,\n",
    "    \t\t\"_has_main_contact\": ModelInstance__has_main_contact,\n",
    "    \t\t\"model format\": ModelInstance_model_format,\n",
    "    \t\t\"model format version compatibility\": ModelInstance_model_format_version_compatibility,\n",
    "            \"model scope\": ModelInstance_model_scope,\n",
    "    \t\t\"species\":ModelInstance_species,\n",
    "    \t\t\"title\": ModelInstance_title,\n",
    "            \"version\": ModelInstance_version\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "data['Person'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": Person__ID,\n",
    "    \t\t\"email\": Person_email,\n",
    "    \t\t\"first name\": Person_first_name,\n",
    "            \"last name\": Person_last_name,\n",
    "    \t\t\"ORCID\": Person_ORCID\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['PLAComponent'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": PLAComponent__ID,\n",
    "    \t\t\"associated task (ID)\": PLAComponent_associated_task_ID,\n",
    "    \t\t\"ID\": PLAComponent_ID,\n",
    "            \"_has_owner\": PLAComponent__has_owner,\n",
    "    \t\t\"phase\": PLAComponent_phase\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "data['Project'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": Project__ID,\n",
    "    \t\t\"_has_coordinator\": Project__has_coordinator,\n",
    "    \t\t\"description\": Project_description,\n",
    "            \"title\": Project_title\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['Publication/Resource'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": PublicationResource__ID,\n",
    "    \t\t\"ID\": PublicationResource_ID,\n",
    "    \t\t\"ID type\": PublicationResource_ID_type\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "data['StudyTarget'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": StudyTarget__ID,\n",
    "    \t\t\"abbreviation\": StudyTarget_abbreviation,\n",
    "    \t\t\"full name\": StudyTarget_full_name,\n",
    "            \"source of name\": StudyTarget_source_of_name,\n",
    "    \t\t\"type\": StudyTarget_type\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "data['Subject'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": Subject__ID,\n",
    "    \t\t\"age\": Subject_age,\n",
    "    \t\t\"age category\": Subject_age_category,\n",
    "    \t\t\"age range (max)\": Subject_age_range_max,\n",
    "            \"age range (min)\": Subject_age_range_min,\n",
    "    \t\t\"alias\": Subject_alias,\n",
    "            \"disability/disease\": Subject_disabilitydisease,\n",
    "            \"genotype\": Subject_genotype,\n",
    "    \t\t\"handedness\": Subject_handedness,\n",
    "    \t\t\"sex\": Subject_sex,\n",
    "    \t\t\"species\": Subject_species,\n",
    "            \"strain\": Subject_strain\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['SubjectGroup'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": SubjectGroup__ID,\n",
    "    \t\t\"age category\": SubjectGroup_age_category,\n",
    "    \t\t\"age range (max)\": SubjectGroup_age_range_max,\n",
    "            \"age range (min)\": SubjectGroup_age_range_min,\n",
    "    \t\t\"alias\": SubjectGroup_alias,\n",
    "    \t\t\"description\": SubjectGroup_description,\n",
    "            \"disability/disease\": SubjectGroup_disabilitydisease,\n",
    "            \"genotype\": SubjectGroup_genotype,\n",
    "    \t\t\"handedness\": SubjectGroup_handedness,\n",
    "    \t\t\"number of subjects\": SubjectGroup_handedness,\n",
    "    \t\t\"sex\": SubjectGroup_sex,\n",
    "    \t\t\"species\": SubjectGroup_species,\n",
    "            \"strain\": SubjectGroup_strain\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['TissueSample'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": TissueSample__ID,\n",
    "    \t\t\"alias\": TissueSample_alias,\n",
    "    \t\t\"hemisphere\": TissueSample_hemisphere,\n",
    "            \"pathology\": TissueSample_pathology,\n",
    "            \"type\": TissueSample_type\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "# Save JSON\n",
    "with open(output_MINDS_JSON, 'w') as ff:\n",
    "    json.dump(data, ff,sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70ce6cd9-4b4c-4f85-9006-8e07a9b4cfef'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the minds_metadata.json to HBP Collab storage\n",
    "collab_storage.upload_file(output_MINDS_JSON, COLLAB_PATH+\"/\"+output_MINDS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}